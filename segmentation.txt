- Segmentation Model

Data Preprocessing Method
We utilized the PyTorch framework and leveraged torchvision's transforms for data preprocessing, which provides convenience and efficiency.
The data processing for the segmentation model was straightforward: we performed image resizing (uniformly cropped to 224×224 pixels) and normalized each channel of the data.
For normalization, we used the mean and standard deviation values from ImageNet. ImageNet is a highly recognized large-scale 2D dataset, making its statistical parameters reliable and widely adopted for transfer learning.

model implementation
We implemented a UNet-like architecture. Through literature review of fundus-related models, we found that many segmentation models employ UNet architectures.
UNet utilizes an encoder-decoder structure that first downsamples to extract feature information, then upsamples to restore the original image dimensions, performing pixel-wise classification.
We designed a four-layer encoder and four-layer decoder, where each layer consists of convolutional blocks and max pooling operations. The convolutional blocks include Conv2d, BatchNorm2d, and ReLU activation functions, which effectively extract features and prevent overfitting.
We employed CrossEntropyLoss as our loss function for multi-class segmentation.

- Results and Discussion
Visualizations
The following visualizations demonstrate our model's performance:
- results_overlay.png: Shows the overlay of predicted segmentation masks on original fundus images
- results_segmentation.png: Displays side-by-side comparison of ground truth and predicted segmentation masks

- Quantitative Metrics
We evaluated our model using standard segmentation metrics: Accuracy, IoU (Intersection over Union), and Dice Score.
Test Loss: 0.0894
Pixel Accuracy: 0.9758 (97.58%)
IoU = TP / (TP + FP + FN)
IoU Score: 0.7010 (70.10%)
Dice = 2×TP / (2×TP + FP + FN) = 2×IoU / (1 + IoU)
Dice Score: 0.8242 (82.42%)

- Analysis of Algorithm/Model Performance
The Dice score achieved our proposal's expectation of exceeding 0.8, indicating strong segmentation performance. The visualization results also demonstrate excellent qualitative outcomes with clear boundary delineation and accurate region identification.
Our model training was highly efficient, requiring only 10 epochs and approximately 20 minutes of training time, demonstrating good convergence properties and computational efficiency.

- Next Steps and Future Work
According to our initial proposal, we planned to implement a Vision Transformer (ViT) for the segmentation task. However, due to our limited familiarity with ViT architectures, we decided to first establish a baseline using CNN-based approaches.
Moving forward, we will implement and compare ViT-based segmentation models to potentially achieve improved performance and explore the effectiveness of attention mechanisms for fundus image segmentation.